{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3887470a",
   "metadata": {},
   "source": [
    "# Using minimizers and the parameter choice in model fitting\n",
    "\n",
    "### Goals:\n",
    "\n",
    "1. To continue fitting models of time-variability to the Vela pulsar data.\n",
    "2. To understand how our choice of model parameter can affect the quality of the results.\n",
    "\n",
    "### Timing\n",
    "\n",
    "1. Try to finish this notebook in 35-40 minutes. \n",
    "\n",
    "### Question and Answer Template\n",
    "\n",
    "You can go to the link below, and do \"file\" -> \"make a copy\" to make yourself a google doc that you can use to fill in the answers to the question in this weeks notebooks.\n",
    "\n",
    "https://docs.google.com/document/d/1rKwI8zIhoj7zSRdIGAYcBkfRnCz1eIrEYE6lMJRqYkU/edit?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e711ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import scipy.optimize as optimize\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea7dc1a",
   "metadata": {},
   "source": [
    "### New functions we will use in this module\n",
    "\n",
    "| Function Name            | What it does |\n",
    "| - | - |\n",
    "| scipy.optimize.minimize  | Find the set of parameters that minimize a function |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64ee46b",
   "metadata": {},
   "source": [
    "### Ok let's pick up where we left off with the Vela pulsar data\n",
    "\n",
    "(This cell is just a repeat of loading the Vela data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b370ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(open(\"../data/Vela_Flux.txt\", 'rb'), usecols=range(7))\n",
    "\n",
    "# This is how we pull out the data from columns in the array.\n",
    "\n",
    "# This is the date in \"Mission Elapsed Time\"\n",
    "# For the Fermi mission, this is defined to be the number of seconds since the start of 2001.\n",
    "date_MET = data[:,0]\n",
    "# This is the offset in seconds between the Fermi \"MET\" and the UNIX \"epoch\" used by matplotlib\n",
    "MET_To_Unix = 978336000\n",
    "\n",
    "# These are the numbers of photons observed from Vela each week in the \"low\" Energy Band (100 MeV - 800 MeV)\n",
    "nObs_LE = data[:,1]\n",
    "\n",
    "# These are the number of photons expected from Vela each week, under the assumption that it is \n",
    "# not varying at all, and the only differences depend on how long we spent looking at Vela\n",
    "# that particular weeek\n",
    "nExp_LE = data[:,2]\n",
    "\n",
    "# These are the band bounds, in MeV\n",
    "LE_bounds = (100., 800.)\n",
    "\n",
    "# This is the \"significance\" of the variation for each week.  We will discuss this more later\n",
    "signif_LE = data[:,3]\n",
    "\n",
    "nObs_HE = data[:,4]\n",
    "nExp_HE = data[:,5]\n",
    "signif_HE = data[:6]\n",
    "HE_bounds = (800., 10000.)\n",
    "\n",
    "# This converts the dates to something that matplotlib understands\n",
    "dates = [datetime.datetime.fromtimestamp(date + MET_To_Unix) for date in date_MET]\n",
    "date_YEAR = 2001 +  (date_MET / (24*3600*365))\n",
    "years_since_mid_2014 = date_YEAR  - 2014.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a565f2dd",
   "metadata": {},
   "source": [
    "### Now let's add the functions we needed to minimize the $\\chi^2$\n",
    "\n",
    "That includes the model, a function to calculate the residuals and a function to compute the $\\chi^2$.\n",
    "\n",
    "These are exactly the same functions that we used last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194cafef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_function(xvals, params):\n",
    "    return params[0] + xvals*params[1]\n",
    "\n",
    "def residual_function(data_x, data_y, model_function, params):\n",
    "    model_y = model_function(data_x, params)\n",
    "    residual = data_y - model_y\n",
    "    return residual\n",
    "\n",
    "def chi2_function(data_x, data_y, data_sigma_y, model_function, params):\n",
    "    model_y = model_function(data_x, params)\n",
    "    chi2 = ((data_y - model_y)/(data_sigma_y))**2\n",
    "    return np.sum(chi2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737b5843",
   "metadata": {},
   "source": [
    "# Function minimizers\n",
    "\n",
    "Finding the set of parameters that minimize a function is a very common thing to do in research. In our case, we are looking for the set of model parameters that give us the smallest $\\chi^2$, i.e., the best fit.\n",
    "\n",
    "Last week, we did this by hand by varying two parameters in a linear model and calculating the $\\chi^2$ for each set of values. \n",
    "\n",
    "Since this is such a common thing to do, there are many software packages that will do it.  Typically they refer to the function that is being minimized as the \"cost function\", and they expect you to provide a function that takes only the model parameters as inputs.\n",
    "\n",
    "So we are going to write a \"cost function\" that just calls our $\\chi^2$ with the right versions of the data and model.  \n",
    "\n",
    "Because the software we use (`scipy.optimize`) has a slightly different convention than what we are using, we will multiply the $\\chi^2$ by a factor of 0.5 so that the uncertainty estimated returned by the minimizer will be correct.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b838578",
   "metadata": {},
   "outputs": [],
   "source": [
    "excess_counts = nObs_LE-nExp_LE\n",
    "sigma_counts =  np.sqrt(nObs_LE)\n",
    "\n",
    "def cost_function(params):\n",
    "    return 0.5*chi2_function(years_since_mid_2014, excess_counts, sigma_counts, linear_function, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff78663",
   "metadata": {},
   "source": [
    "### Invoking the minimizer and looking at the result\n",
    "\n",
    "First we are going to invoke the minimizer in the next cell.\n",
    "\n",
    "It is worth spending a bit of time thinking about what it is doing.\n",
    "\n",
    "Note that we pass three things to the minimizer:\n",
    "   1. the `cost_function` (you might not have explicitly noticed this before, but we can pass functions to other functions)\n",
    "   2. an initial guess as to the parameter values.  In this case we will start with (0., 0.), i.e., slope and offset are both zero.\n",
    "   3. The method of minimization. There are many algorithms that have been developed to minimize functions. Here we choose one of the speedier algorithms, but you can specify different algorithms (see [scipy reference](https://docs.scipy.org/doc/scipy/reference/optimize.html)).\n",
    "   \n",
    "And note that the minimizer returns a `result` to us, which we will explore in the second cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a8be73",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = optimize.minimize(cost_function, x0=[0., 0.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e105b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = optimize.minimize(cost_function, [0., 0.], method=\"BFGS\")\n",
    "pars = result['x']\n",
    "fmin = result['fun']\n",
    "p0_best = pars[0]\n",
    "p1_best = pars[1]\n",
    "cov = result['hess_inv']\n",
    "p0_err = np.sqrt(cov[0,0])\n",
    "p1_err = np.sqrt(cov[1,1])\n",
    "correl = cov[1,0]/np.sqrt(p0_err*p1_err)\n",
    "\n",
    "print(\"Fitter result:\")\n",
    "print(result)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Human readable version ---------------\")\n",
    "print(f\"  p0 best fit value: {p0_best:.1f} ± {p0_err:.1f} counts\")\n",
    "print(f\"  p1 best fit value: {p1_best:.1f} ± {p1_err:.1f} counts / year\")\n",
    "print(f\"  Minimum value of cost function: {fmin:.1f}\")\n",
    "print(f\"  Minimum value of chi**2: {(2*fmin):.1f}\")\n",
    "print(f\"  Correlation between p0 and p1: {correl:.2f}\")\n",
    "print(f\"  Number of times cost function was evaluated to find minimum: {result['nfev']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45d8d5f",
   "metadata": {},
   "source": [
    "### Ok, now let's draw the fit result on top of the contours we made last week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989b9ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(r\"$p_0$ = Offset [counts]\")\n",
    "plt.ylabel(r\"$p_1$ = Slope [counts/year]\")\n",
    "\n",
    "\n",
    "nx = 51\n",
    "ny = 51\n",
    "\n",
    "params = np.array([0., 0.])\n",
    "chi2_2d_scan_vals = np.zeros((nx, ny))\n",
    "offset_scan_points = np.linspace(-10., 10., nx)\n",
    "slope_scan_points = np.linspace(-5., 5., ny)\n",
    "\n",
    "# Double loop for 2d scan\n",
    "for i in range(nx):\n",
    "    params[0] = offset_scan_points[i]\n",
    "    for j in range(ny):\n",
    "        params[1] = slope_scan_points[j]\n",
    "        chi2_2d_scan_vals[i,j] = chi2_function(years_since_mid_2014, excess_counts, sigma_counts,\n",
    "                                               linear_function, params)\n",
    "\n",
    "min_chi2 = chi2_2d_scan_vals.min()\n",
    "chi2_2d_scan_vals -= min_chi2\n",
    "\n",
    "# Now let's plot it.\n",
    "plt.imshow(chi2_2d_scan_vals.T, extent=(-10, 10, -5, 5), aspect='auto', origin='lower')\n",
    "plt.colorbar(label=r\"$\\Delta \\chi^2$\")\n",
    "plt.contour(offset_scan_points, slope_scan_points, chi2_2d_scan_vals.T, levels=[1, 4, 9], colors=\"white\")\n",
    "plt.errorbar(p0_best, p1_best, xerr=p0_err, yerr=p1_err, color='yellow')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085c4748",
   "metadata": {},
   "source": [
    "### Questions for discusion\n",
    "\n",
    "#### 1.1  Let's make sure that you understand what we have done so far. Describe in some detail the relationship between the colormap, the white contours and the yellow error bars. What does each thing represent? How do they correspond to the information in the \"human readable\" printout of the result?  (Note that this is the same colormap that we made at the end of last week's second notebook when we did the two dimensional scan over the parameter values.)\n",
    "\n",
    "#### 1.2 To make the colormap we had to do a double loop over a grid of points for the parameters $p_0$ and $p_1$. The grid was 51x51, for a total of 2601 points, meaning we had to evaluate the cost function 2601 times. In this example we only had 2 parameters; imagine instead that we had 3 or 4 parameters. How would that affect the time it took to evaluate the cost function on a grid over all the parameters? Compare that to the number of calls that the fitter makes in order to find the minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c18be9",
   "metadata": {},
   "source": [
    "### Reproducibility\n",
    "\n",
    "Let's play around with the fitter and try it with 20 different initial guesses and compare the results and the number of calls to the cost function it takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c522c906",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    x0 = [np.random.uniform(-10, 10), np.random.uniform(-5, 5)]\n",
    "    result = optimize.minimize(cost_function, x0=x0)\n",
    "    best_fit = result['x']\n",
    "    print(f\"Initial guess: ({x0[0]:.2f} {x0[1]:.2f}): found fit result: ({best_fit[0]:.6f} {best_fit[1]:.6f}) after {result['nfev']} calls to the cost function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241edda8",
   "metadata": {},
   "source": [
    "### Question for discussion\n",
    "\n",
    "#### 2.1 The results for the 20 trials are very similar but not quite identical. Does this make sense to you? Do you have an idea about why the results aren't identical? How do you think the fitter decides that it is done? (Don't worry if you don't know the answer, just have a guess.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1150fbb",
   "metadata": {},
   "source": [
    "# Correlation between model parameters\n",
    "\n",
    "In last week's notebook, you may have noted that we set t=0 to be in the middle of 2014, which might seem arbitrary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde23d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "years_since_mid_2014 = date_YEAR  - 2014.5\n",
    "excess_counts = nObs_LE-nExp_LE\n",
    "plt.scatter(years_since_mid_2014, excess_counts)\n",
    "plt.xlabel(r\"Time since mid 2014 [years]\")\n",
    "plt.ylabel(r\"$n_{\\rm obs}$ [per week]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c8ad86",
   "metadata": {},
   "source": [
    "There was actually a very good reason to do that, which we will examine now. \n",
    "\n",
    "The Fermi telescope actually launched in 2008, so we could set t=0 to be in January 2008."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0589152a",
   "metadata": {},
   "outputs": [],
   "source": [
    "years_since_2008 = date_YEAR  - 2008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacf45a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "excess_counts = nObs_LE-nExp_LE\n",
    "plt.scatter(years_since_2008, excess_counts)\n",
    "plt.xlabel(r\"Time since 2008 [years]\")\n",
    "plt.ylabel(r\"$n_{\\rm ex}$ [per week]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253c3386",
   "metadata": {},
   "source": [
    "Now, let's think about what happens to fitting our linear model when we make this change. \n",
    "\n",
    "Keep in mind that the model function we are using is a simple equation for a line: $y = p_0 + p_1 x$\n",
    "\n",
    "By moving the zero point of the x-axis, we are changing how that function works with the data. The model parameters are now more *correlated*. To demonstrate this, we will make a plot for different values of $p_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2cbbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(r\"Time since 2008 [years]\")\n",
    "plt.ylabel(r\"$n_{\\rm ex}$ [per week]\")\n",
    "plt.errorbar(years_since_2008, excess_counts, yerr=sigma_counts, alpha=0.2)\n",
    "\n",
    "xvals = years_since_2008\n",
    "params = np.array([0., 0.])\n",
    "for slope in np.linspace(-15, 15, 5):\n",
    "    params[1] = slope\n",
    "    plt.plot(xvals, linear_function(xvals, params), label=rf\"Slope = {slope:0.1f}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641cd9f3",
   "metadata": {},
   "source": [
    "As you can see, all the lines cross at t=0, which is now off the left side of the plot.  Before it was more or less in the middle of the plot.  \n",
    "\n",
    "**What this means is that if you were to pick a value like, say, $p_1 = 7.5$ the model tends to be above the average of the data for the entire time.  This means that you would have to change the offset $p_0$ to a negative number to compensate.**\n",
    "\n",
    "That is exactly what we mean when we say that the parameters have become more correlated. \n",
    "\n",
    "Let's explore this with the minimizer.\n",
    "\n",
    "First we have to make a version of the cost function that uses this version of the x-axis data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75531a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function_bad(params):\n",
    "    return 0.5*chi2_function(years_since_2008, excess_counts, sigma_counts, linear_function, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5bcaca",
   "metadata": {},
   "source": [
    "Now, let's minimize the cost function again to find the best fit parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafa389e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_bad = optimize.minimize(cost_function_bad, [0., 0.])\n",
    "pars_bad = result_bad['x']\n",
    "fmin_bad = result_bad['fun']\n",
    "p0_best_bad = pars_bad[0]\n",
    "p1_best_bad = pars_bad[1]\n",
    "cov_bad = result_bad['hess_inv']\n",
    "p0_err_bad = np.sqrt(cov_bad[0,0])\n",
    "p1_err_bad = np.sqrt(cov_bad[1,1])\n",
    "correl_bad = cov_bad[1,0]/(p0_err_bad*p1_err_bad)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Human readable version: bad idea ---------------\")\n",
    "print(f\"  Minimum value of cost function: {fmin_bad:.1f}\")\n",
    "print(f\"  Minimum value of chi**2: {(2*fmin_bad):.1f}\")\n",
    "print(f\"  p0 best fit: {p0_best_bad:.1f} ± {p0_err_bad:.1f} counts\")\n",
    "print(f\"  p1 best fit: {p1_best_bad:.1f} ± {p1_err_bad:.1f} counts / year\")\n",
    "print(f\"  Correlation between p0 and p1: {correl_bad:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ee1cc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.xlabel(r\"$p_0$ = Offset [counts]\")\n",
    "plt.ylabel(r\"$p_1$ = Slope [counts/year]\")\n",
    "\n",
    "chi2_2d_scan_vals_bad = np.zeros((nx, ny))\n",
    "offset_scan_points = np.linspace(-10., 10., nx)\n",
    "slope_scan_points = np.linspace(-5., 5., ny)\n",
    "\n",
    "# Double loop for 2d scan\n",
    "for i in range(nx):\n",
    "    params[0] = offset_scan_points[i]\n",
    "    for j in range(ny):\n",
    "        params[1] = slope_scan_points[j]\n",
    "        chi2_2d_scan_vals_bad[i,j] = chi2_function(years_since_2008, excess_counts, sigma_counts,\n",
    "                                                   linear_function, params)\n",
    "\n",
    "min_chi2_bad = chi2_2d_scan_vals_bad.min()\n",
    "chi2_2d_scan_vals_bad -= min_chi2_bad\n",
    "\n",
    "# Now let's plot it.\n",
    "plt.imshow(chi2_2d_scan_vals_bad.T, extent=(-10, 10, -5, 5), aspect='auto', origin='lower')\n",
    "plt.colorbar(label=r\"$\\Delta \\chi^2$\")\n",
    "plt.contour(offset_scan_points, slope_scan_points, chi2_2d_scan_vals_bad.T, levels=[1, 4, 9], colors=\"white\")\n",
    "plt.errorbar(p0_best_bad, p1_best_bad, xerr=p0_err_bad, yerr=p1_err_bad, color='cyan')\n",
    "#plt.errorbar(p0_best, p1_best, xerr=p0_err, yerr=p1_err, color='yellow')\n",
    "plt.show()\n",
    "\n",
    "print(f\"New best fit value is {min_chi2_bad:0.1f} for ({p0_best_bad:0.1f} ± {p0_err_bad:0.1f}, {p1_best_bad:0.1f} ± {p1_err_bad:0.1f})\")\n",
    "\n",
    "print(f\"Original fit value was {min_chi2:0.1f} for ({p0_best:0.1f} ± {p0_err:0.1f}, {p1_best:0.1f} ± {p1_err:0.1f})\")\n",
    "\n",
    "print(f\"Original correlation was {correl:0.2f}, now it is {correl_bad:0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e153863f",
   "metadata": {},
   "source": [
    "### Questions for discussion\n",
    "\n",
    "#### 3.1 What is going on in this plot?  Why are the contours tilted?  Why are the error bars larger?  \n",
    "\n",
    "#### 3.2 What does this tell us about what we should consider when building models?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
